{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#/content/wine.csv\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import math\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class WineData(Dataset):\n",
        "  def __init__(self):\n",
        "    #super(WineData,self).__init__()\n",
        "    xy = np.loadtxt(\"/content/wine.csv\",delimiter=\",\",dtype=np.float32,skiprows=1)\n",
        "    self.x = torch.from_numpy(xy[:,1:])\n",
        "    self.y = torch.from_numpy((xy[:,[0]]).astype(np.int64)).squeeze(1)-1 #[4,1] -> 4 & classes:[1,2,3] -> [0,1,2]\n",
        "    self.n_samples = xy.shape[0]\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    return self.x[index],self.y[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_samples\n",
        "\n",
        "datset = WineData()\n",
        "dataloader = DataLoader(dataset=datset,batch_size=10,shuffle=True,num_workers=2)\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "  def __init__(self,in_features,hidden_size,out_features):\n",
        "    super(NeuralNet,self).__init__()\n",
        "    self.l1 = nn.Linear(in_features,hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.l2 = nn.Linear(hidden_size,out_features)\n",
        "\n",
        "  def forward(self,x):\n",
        "    pred = self.l1(x)\n",
        "    pred = self.relu(pred)\n",
        "    pred = self.l2(pred)\n",
        "    return pred\n",
        "\n",
        "in_features = 13\n",
        "hidden_size = 20\n",
        "num_epochs = 2\n",
        "out_features = 3\n",
        "model = NeuralNet(in_features,hidden_size,out_features)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=0.01)\n",
        "\n",
        "for e in range(num_epochs):\n",
        "  for smp,lbl in dataloader:\n",
        "    pred = model(smp)\n",
        "    l = loss(pred,lbl)\n",
        "    optimizer.zero_grad()\n",
        "    l.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "with torch.no_grad():\n",
        "  pred = model(torch.tensor([[14.39,1.87,2.45,14.6,96,2.5,2.52,0.3,1.98,5.25,1.02,3.58,1290]],dtype=torch.float32))\n",
        "  _,pred = torch.max(pred,1)\n",
        "  print(pred+1)"
      ],
      "metadata": {
        "id": "zD8qXoNJu2bA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df3d030d-8e4b-493b-8936-8222483d7813"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "\n",
        "in_features = 784\n",
        "hidden_size = 100\n",
        "out_features = 10\n",
        "num_epochs = 2\n",
        "batch_size = 100\n",
        "l_rt = 0.001\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root=\"./data\",train=True,transform=transforms.ToTensor(),download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root=\"./data\",train=False,transform=transforms.ToTensor())\n",
        "\n",
        "trn_loader = DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)\n",
        "tst_loader = DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False)\n",
        "\n",
        "dataiter = iter(trn_loader)\n",
        "smp,lbl = next(dataiter)\n",
        "print(smp.shape,lbl.shape)\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "  def __init__(self,in_features,hidden_size,out_features):\n",
        "    super(NeuralNet,self).__init__()\n",
        "    self.l1 = nn.Linear(in_features,hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.l2 = nn.Linear(hidden_size,out_features)\n",
        "\n",
        "  def forward(self,x):\n",
        "    pred = self.l1(x)\n",
        "    pred = self.relu(pred)\n",
        "    pred = self.l2(pred)\n",
        "    return pred\n",
        "\n",
        "model = NeuralNet(in_features,hidden_size,out_features)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=l_rt)\n",
        "\n",
        "for e in range(num_epochs):\n",
        "  for smp,lbl in trn_loader:\n",
        "    smp = smp.reshape(-1,28*28)\n",
        "    pred = model(smp)\n",
        "    l = loss(pred,lbl)\n",
        "    optimizer.zero_grad()\n",
        "    l.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "with torch.no_grad():\n",
        "  n_cor = 0\n",
        "  n_sap = 0\n",
        "  for img,lbl in tst_loader:\n",
        "    img = img.reshape(-1,28*28)\n",
        "    pred = model(img)\n",
        "    _,pred = torch.max(pred.data,1)\n",
        "    n_sap += lbl.shape[0]\n",
        "    n_cor += (pred==lbl).sum().item()\n",
        "  acc = 100.0*n_cor/n_sap\n",
        "  print(acc)"
      ],
      "metadata": {
        "id": "GgLsLA8V25UD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e43a516-c379-4ce4-c23a-a61249b54f20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 16.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 488kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.74MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.24MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
            "95.26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Bad CNN only for learning how to setup a CNN\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Hyper-parameters\n",
        "num_epochs = 1\n",
        "batch_size = 200\n",
        "learning_rate = 0.001\n",
        "\n",
        "# dataset has PILImage images of range [0, 1].\n",
        "# We transform them to Tensors of normalized range [-1, 1]\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size,shuffle=False)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "class MyCnn(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MyCnn,self).__init__()\n",
        "    self.cl1 = nn.Conv2d(3,8,5)\n",
        "    self.pl1 = nn.MaxPool2d(4,4)\n",
        "    self.cl2 = nn.Conv2d(8,16,5)\n",
        "    self.pl2 = nn.MaxPool2d(2,2)\n",
        "    self.act = nn.ReLU()\n",
        "    self.fc1 = nn.Linear(16,100)\n",
        "    self.fc2 = nn.Linear(100,70)\n",
        "    self.fc3 = nn.Linear(70,10)\n",
        "\n",
        "  def forward(self,x):\n",
        "    pred = self.cl1(x)\n",
        "    pred = self.act(pred)\n",
        "    pred = self.pl1(pred)\n",
        "    pred = self.cl2(pred)\n",
        "    pred = self.act(pred)\n",
        "    pred = self.pl2(pred)\n",
        "    pred = pred.reshape(pred.shape[0],-1)\n",
        "    #print(pred.shape)\n",
        "    pred = self.fc1(pred)\n",
        "    pred = self.act(pred)\n",
        "    pred = self.fc2(pred)\n",
        "    pred = self.act(pred)\n",
        "    pred = self.fc3(pred)\n",
        "    return pred #no softmax because CrossEntropyLoss() automatically does it\n",
        "\n",
        "model = MyCnn()\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)\n",
        "\n",
        "for e in range(num_epochs):\n",
        "  for i,(smp,lbl) in enumerate(train_loader):\n",
        "    pred = model(smp)\n",
        "    l = loss(pred,lbl)\n",
        "    optimizer.zero_grad()\n",
        "    l.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"PIter = {i}, Loss = {l}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "  n_correct = 0\n",
        "  n_samples = 0\n",
        "  for images, labels in test_loader:\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      n_samples += labels.size(0)\n",
        "      n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "  acc = 100.0 * n_correct / n_samples\n",
        "  print(f'Accuracy of the network: {acc} %')"
      ],
      "metadata": {
        "id": "th14LsuW0212",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a01c8e25-5abe-4575-ff91-73614dbded06"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "PIter = 0, Loss = 2.3043785095214844\n",
            "PIter = 1, Loss = 2.298813819885254\n",
            "PIter = 2, Loss = 2.306138515472412\n",
            "PIter = 3, Loss = 2.302088975906372\n",
            "PIter = 4, Loss = 2.296060085296631\n",
            "PIter = 5, Loss = 2.3052196502685547\n",
            "PIter = 6, Loss = 2.304271697998047\n",
            "PIter = 7, Loss = 2.299051284790039\n",
            "PIter = 8, Loss = 2.3038084506988525\n",
            "PIter = 9, Loss = 2.305213689804077\n",
            "PIter = 10, Loss = 2.3099753856658936\n",
            "PIter = 11, Loss = 2.2987382411956787\n",
            "PIter = 12, Loss = 2.3074991703033447\n",
            "PIter = 13, Loss = 2.296194076538086\n",
            "PIter = 14, Loss = 2.3068344593048096\n",
            "PIter = 15, Loss = 2.3022103309631348\n",
            "PIter = 16, Loss = 2.307185411453247\n",
            "PIter = 17, Loss = 2.300088882446289\n",
            "PIter = 18, Loss = 2.3014018535614014\n",
            "PIter = 19, Loss = 2.3007352352142334\n",
            "PIter = 20, Loss = 2.299029588699341\n",
            "PIter = 21, Loss = 2.304980993270874\n",
            "PIter = 22, Loss = 2.3138866424560547\n",
            "PIter = 23, Loss = 2.3060178756713867\n",
            "PIter = 24, Loss = 2.307762861251831\n",
            "PIter = 25, Loss = 2.3021316528320312\n",
            "PIter = 26, Loss = 2.312452554702759\n",
            "PIter = 27, Loss = 2.2981438636779785\n",
            "PIter = 28, Loss = 2.3072152137756348\n",
            "PIter = 29, Loss = 2.3094899654388428\n",
            "PIter = 30, Loss = 2.3001961708068848\n",
            "PIter = 31, Loss = 2.304537296295166\n",
            "PIter = 32, Loss = 2.3059630393981934\n",
            "PIter = 33, Loss = 2.306309938430786\n",
            "PIter = 34, Loss = 2.3112289905548096\n",
            "PIter = 35, Loss = 2.3044214248657227\n",
            "PIter = 36, Loss = 2.295497179031372\n",
            "PIter = 37, Loss = 2.3122360706329346\n",
            "PIter = 38, Loss = 2.2997536659240723\n",
            "PIter = 39, Loss = 2.3204233646392822\n",
            "PIter = 40, Loss = 2.3100996017456055\n",
            "PIter = 41, Loss = 2.307460069656372\n",
            "PIter = 42, Loss = 2.307919502258301\n",
            "PIter = 43, Loss = 2.314531087875366\n",
            "PIter = 44, Loss = 2.309610605239868\n",
            "PIter = 45, Loss = 2.308884620666504\n",
            "PIter = 46, Loss = 2.2999513149261475\n",
            "PIter = 47, Loss = 2.3085970878601074\n",
            "PIter = 48, Loss = 2.3011019229888916\n",
            "PIter = 49, Loss = 2.3044910430908203\n",
            "PIter = 50, Loss = 2.303110122680664\n",
            "PIter = 51, Loss = 2.309997797012329\n",
            "PIter = 52, Loss = 2.302457809448242\n",
            "PIter = 53, Loss = 2.3006489276885986\n",
            "PIter = 54, Loss = 2.3113324642181396\n",
            "PIter = 55, Loss = 2.304680585861206\n",
            "PIter = 56, Loss = 2.293158769607544\n",
            "PIter = 57, Loss = 2.3080930709838867\n",
            "PIter = 58, Loss = 2.3086187839508057\n",
            "PIter = 59, Loss = 2.3066043853759766\n",
            "PIter = 60, Loss = 2.3054628372192383\n",
            "PIter = 61, Loss = 2.301950216293335\n",
            "PIter = 62, Loss = 2.3084616661071777\n",
            "PIter = 63, Loss = 2.311497688293457\n",
            "PIter = 64, Loss = 2.3089210987091064\n",
            "PIter = 65, Loss = 2.3046813011169434\n",
            "PIter = 66, Loss = 2.307537078857422\n",
            "PIter = 67, Loss = 2.31123948097229\n",
            "PIter = 68, Loss = 2.3090927600860596\n",
            "PIter = 69, Loss = 2.3068032264709473\n",
            "PIter = 70, Loss = 2.3066060543060303\n",
            "PIter = 71, Loss = 2.2969653606414795\n",
            "PIter = 72, Loss = 2.310901165008545\n",
            "PIter = 73, Loss = 2.301342248916626\n",
            "PIter = 74, Loss = 2.3084516525268555\n",
            "PIter = 75, Loss = 2.2977676391601562\n",
            "PIter = 76, Loss = 2.299663782119751\n",
            "PIter = 77, Loss = 2.2949941158294678\n",
            "PIter = 78, Loss = 2.302973747253418\n",
            "PIter = 79, Loss = 2.3030762672424316\n",
            "PIter = 80, Loss = 2.309333086013794\n",
            "PIter = 81, Loss = 2.3024706840515137\n",
            "PIter = 82, Loss = 2.2994191646575928\n",
            "PIter = 83, Loss = 2.319012403488159\n",
            "PIter = 84, Loss = 2.306340217590332\n",
            "PIter = 85, Loss = 2.306647300720215\n",
            "PIter = 86, Loss = 2.3057892322540283\n",
            "PIter = 87, Loss = 2.3056068420410156\n",
            "PIter = 88, Loss = 2.3013837337493896\n",
            "PIter = 89, Loss = 2.315258264541626\n",
            "PIter = 90, Loss = 2.310105800628662\n",
            "PIter = 91, Loss = 2.309762716293335\n",
            "PIter = 92, Loss = 2.299511671066284\n",
            "PIter = 93, Loss = 2.301119327545166\n",
            "PIter = 94, Loss = 2.2984774112701416\n",
            "PIter = 95, Loss = 2.298436403274536\n",
            "PIter = 96, Loss = 2.3069698810577393\n",
            "PIter = 97, Loss = 2.3025267124176025\n",
            "PIter = 98, Loss = 2.30300235748291\n",
            "PIter = 99, Loss = 2.295440435409546\n",
            "PIter = 100, Loss = 2.3008835315704346\n",
            "PIter = 101, Loss = 2.3070878982543945\n",
            "PIter = 102, Loss = 2.3083267211914062\n",
            "PIter = 103, Loss = 2.299689769744873\n",
            "PIter = 104, Loss = 2.307222843170166\n",
            "PIter = 105, Loss = 2.305095672607422\n",
            "PIter = 106, Loss = 2.3101742267608643\n",
            "PIter = 107, Loss = 2.305184841156006\n",
            "PIter = 108, Loss = 2.296757459640503\n",
            "PIter = 109, Loss = 2.3039839267730713\n",
            "PIter = 110, Loss = 2.302712917327881\n",
            "PIter = 111, Loss = 2.3060173988342285\n",
            "PIter = 112, Loss = 2.302340507507324\n",
            "PIter = 113, Loss = 2.3061721324920654\n",
            "PIter = 114, Loss = 2.3042633533477783\n",
            "PIter = 115, Loss = 2.312817096710205\n",
            "PIter = 116, Loss = 2.3161814212799072\n",
            "PIter = 117, Loss = 2.309983015060425\n",
            "PIter = 118, Loss = 2.3030595779418945\n",
            "PIter = 119, Loss = 2.2990200519561768\n",
            "PIter = 120, Loss = 2.303680658340454\n",
            "PIter = 121, Loss = 2.3026955127716064\n",
            "PIter = 122, Loss = 2.30169415473938\n",
            "PIter = 123, Loss = 2.3003835678100586\n",
            "PIter = 124, Loss = 2.3051886558532715\n",
            "PIter = 125, Loss = 2.2942254543304443\n",
            "PIter = 126, Loss = 2.3010101318359375\n",
            "PIter = 127, Loss = 2.3062026500701904\n",
            "PIter = 128, Loss = 2.3046202659606934\n",
            "PIter = 129, Loss = 2.3037149906158447\n",
            "PIter = 130, Loss = 2.296954393386841\n",
            "PIter = 131, Loss = 2.3052573204040527\n",
            "PIter = 132, Loss = 2.3063721656799316\n",
            "PIter = 133, Loss = 2.3080573081970215\n",
            "PIter = 134, Loss = 2.309037208557129\n",
            "PIter = 135, Loss = 2.307694911956787\n",
            "PIter = 136, Loss = 2.2994565963745117\n",
            "PIter = 137, Loss = 2.2951107025146484\n",
            "PIter = 138, Loss = 2.305417060852051\n",
            "PIter = 139, Loss = 2.305131435394287\n",
            "PIter = 140, Loss = 2.303736686706543\n",
            "PIter = 141, Loss = 2.2979986667633057\n",
            "PIter = 142, Loss = 2.303610324859619\n",
            "PIter = 143, Loss = 2.3045523166656494\n",
            "PIter = 144, Loss = 2.3023345470428467\n",
            "PIter = 145, Loss = 2.3035004138946533\n",
            "PIter = 146, Loss = 2.302548408508301\n",
            "PIter = 147, Loss = 2.306286334991455\n",
            "PIter = 148, Loss = 2.301621675491333\n",
            "PIter = 149, Loss = 2.2922823429107666\n",
            "PIter = 150, Loss = 2.3098161220550537\n",
            "PIter = 151, Loss = 2.298996686935425\n",
            "PIter = 152, Loss = 2.3146510124206543\n",
            "PIter = 153, Loss = 2.3003156185150146\n",
            "PIter = 154, Loss = 2.3057398796081543\n",
            "PIter = 155, Loss = 2.3033783435821533\n",
            "PIter = 156, Loss = 2.3043196201324463\n",
            "PIter = 157, Loss = 2.3043859004974365\n",
            "PIter = 158, Loss = 2.3066036701202393\n",
            "PIter = 159, Loss = 2.310858726501465\n",
            "PIter = 160, Loss = 2.3044893741607666\n",
            "PIter = 161, Loss = 2.2974278926849365\n",
            "PIter = 162, Loss = 2.3029909133911133\n",
            "PIter = 163, Loss = 2.3080523014068604\n",
            "PIter = 164, Loss = 2.306445360183716\n",
            "PIter = 165, Loss = 2.308535575866699\n",
            "PIter = 166, Loss = 2.2973928451538086\n",
            "PIter = 167, Loss = 2.3059158325195312\n",
            "PIter = 168, Loss = 2.3081116676330566\n",
            "PIter = 169, Loss = 2.30428147315979\n",
            "PIter = 170, Loss = 2.3050787448883057\n",
            "PIter = 171, Loss = 2.3066132068634033\n",
            "PIter = 172, Loss = 2.309246301651001\n",
            "PIter = 173, Loss = 2.304410934448242\n",
            "PIter = 174, Loss = 2.297701835632324\n",
            "PIter = 175, Loss = 2.3045270442962646\n",
            "PIter = 176, Loss = 2.304298162460327\n",
            "PIter = 177, Loss = 2.299142360687256\n",
            "PIter = 178, Loss = 2.3085665702819824\n",
            "PIter = 179, Loss = 2.3061532974243164\n",
            "PIter = 180, Loss = 2.3081624507904053\n",
            "PIter = 181, Loss = 2.301527976989746\n",
            "PIter = 182, Loss = 2.2988555431365967\n",
            "PIter = 183, Loss = 2.306363582611084\n",
            "PIter = 184, Loss = 2.3086791038513184\n",
            "PIter = 185, Loss = 2.302042007446289\n",
            "PIter = 186, Loss = 2.302312135696411\n",
            "PIter = 187, Loss = 2.3131167888641357\n",
            "PIter = 188, Loss = 2.3175840377807617\n",
            "PIter = 189, Loss = 2.31000018119812\n",
            "PIter = 190, Loss = 2.3021955490112305\n",
            "PIter = 191, Loss = 2.3047666549682617\n",
            "PIter = 192, Loss = 2.297797203063965\n",
            "PIter = 193, Loss = 2.3072125911712646\n",
            "PIter = 194, Loss = 2.3038604259490967\n",
            "PIter = 195, Loss = 2.300009250640869\n",
            "PIter = 196, Loss = 2.302738904953003\n",
            "PIter = 197, Loss = 2.3039541244506836\n",
            "PIter = 198, Loss = 2.3018784523010254\n",
            "PIter = 199, Loss = 2.3088393211364746\n",
            "PIter = 200, Loss = 2.299656629562378\n",
            "PIter = 201, Loss = 2.306593894958496\n",
            "PIter = 202, Loss = 2.3023111820220947\n",
            "PIter = 203, Loss = 2.3152592182159424\n",
            "PIter = 204, Loss = 2.3009743690490723\n",
            "PIter = 205, Loss = 2.3051223754882812\n",
            "PIter = 206, Loss = 2.3056273460388184\n",
            "PIter = 207, Loss = 2.3093924522399902\n",
            "PIter = 208, Loss = 2.302210569381714\n",
            "PIter = 209, Loss = 2.3065640926361084\n",
            "PIter = 210, Loss = 2.3099896907806396\n",
            "PIter = 211, Loss = 2.308579444885254\n",
            "PIter = 212, Loss = 2.311569929122925\n",
            "PIter = 213, Loss = 2.300708055496216\n",
            "PIter = 214, Loss = 2.3059394359588623\n",
            "PIter = 215, Loss = 2.3032360076904297\n",
            "PIter = 216, Loss = 2.3051416873931885\n",
            "PIter = 217, Loss = 2.2964088916778564\n",
            "PIter = 218, Loss = 2.3000943660736084\n",
            "PIter = 219, Loss = 2.3019793033599854\n",
            "PIter = 220, Loss = 2.307450294494629\n",
            "PIter = 221, Loss = 2.312289237976074\n",
            "PIter = 222, Loss = 2.3089849948883057\n",
            "PIter = 223, Loss = 2.3102867603302\n",
            "PIter = 224, Loss = 2.3076512813568115\n",
            "PIter = 225, Loss = 2.3008813858032227\n",
            "PIter = 226, Loss = 2.2908201217651367\n",
            "PIter = 227, Loss = 2.303927421569824\n",
            "PIter = 228, Loss = 2.3128528594970703\n",
            "PIter = 229, Loss = 2.308166027069092\n",
            "PIter = 230, Loss = 2.309487819671631\n",
            "PIter = 231, Loss = 2.3100059032440186\n",
            "PIter = 232, Loss = 2.304581642150879\n",
            "PIter = 233, Loss = 2.309439182281494\n",
            "PIter = 234, Loss = 2.309582233428955\n",
            "PIter = 235, Loss = 2.306584119796753\n",
            "PIter = 236, Loss = 2.3065714836120605\n",
            "PIter = 237, Loss = 2.3039674758911133\n",
            "PIter = 238, Loss = 2.313990354537964\n",
            "PIter = 239, Loss = 2.2962357997894287\n",
            "PIter = 240, Loss = 2.3044958114624023\n",
            "PIter = 241, Loss = 2.305725336074829\n",
            "PIter = 242, Loss = 2.3052823543548584\n",
            "PIter = 243, Loss = 2.3057913780212402\n",
            "PIter = 244, Loss = 2.299309492111206\n",
            "PIter = 245, Loss = 2.3022656440734863\n",
            "PIter = 246, Loss = 2.300079107284546\n",
            "PIter = 247, Loss = 2.3127944469451904\n",
            "PIter = 248, Loss = 2.2982006072998047\n",
            "PIter = 249, Loss = 2.314798355102539\n",
            "Accuracy of the network: 12.11 %\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}