{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#/content/wine.csv\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import math\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class WineData(Dataset):\n",
        "  def __init__(self):\n",
        "    #super(WineData,self).__init__()\n",
        "    xy = np.loadtxt(\"/content/wine.csv\",delimiter=\",\",dtype=np.float32,skiprows=1)\n",
        "    self.x = torch.from_numpy(xy[:,1:])\n",
        "    self.y = torch.from_numpy((xy[:,[0]]).astype(np.int64)).squeeze(1)-1 #[4,1] -> 4 & classes:[1,2,3] -> [0,1,2]\n",
        "    self.n_samples = xy.shape[0]\n",
        "\n",
        "  def __getitem__(self,index):\n",
        "    return self.x[index],self.y[index]\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.n_samples\n",
        "\n",
        "datset = WineData()\n",
        "dataloader = DataLoader(dataset=datset,batch_size=10,shuffle=True,num_workers=2)\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "  def __init__(self,in_features,hidden_size,out_features):\n",
        "    super(NeuralNet,self).__init__()\n",
        "    self.l1 = nn.Linear(in_features,hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.l2 = nn.Linear(hidden_size,out_features)\n",
        "\n",
        "  def forward(self,x):\n",
        "    pred = self.l1(x)\n",
        "    pred = self.relu(pred)\n",
        "    pred = self.l2(pred)\n",
        "    return pred\n",
        "\n",
        "in_features = 13\n",
        "hidden_size = 20\n",
        "num_epochs = 2\n",
        "out_features = 3\n",
        "model = NeuralNet(in_features,hidden_size,out_features)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(),lr=0.01)\n",
        "\n",
        "for e in range(num_epochs):\n",
        "  for smp,lbl in dataloader:\n",
        "    pred = model(smp)\n",
        "    l = loss(pred,lbl)\n",
        "    optimizer.zero_grad()\n",
        "    l.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "with torch.no_grad():\n",
        "  pred = model(torch.tensor([[14.39,1.87,2.45,14.6,96,2.5,2.52,0.3,1.98,5.25,1.02,3.58,1290]],dtype=torch.float32))\n",
        "  _,pred = torch.max(pred,1)\n",
        "  print(pred+1)"
      ],
      "metadata": {
        "id": "zD8qXoNJu2bA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df3d030d-8e4b-493b-8936-8222483d7813"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "\n",
        "in_features = 784\n",
        "hidden_size = 100\n",
        "out_features = 10\n",
        "num_epochs = 2\n",
        "batch_size = 100\n",
        "l_rt = 0.001\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root=\"./data\",train=True,transform=transforms.ToTensor(),download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root=\"./data\",train=False,transform=transforms.ToTensor())\n",
        "\n",
        "trn_loader = DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)\n",
        "tst_loader = DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False)\n",
        "\n",
        "dataiter = iter(trn_loader)\n",
        "smp,lbl = next(dataiter)\n",
        "print(smp.shape,lbl.shape)\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "  def __init__(self,in_features,hidden_size,out_features):\n",
        "    super(NeuralNet,self).__init__()\n",
        "    self.l1 = nn.Linear(in_features,hidden_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.l2 = nn.Linear(hidden_size,out_features)\n",
        "\n",
        "  def forward(self,x):\n",
        "    pred = self.l1(x)\n",
        "    pred = self.relu(pred)\n",
        "    pred = self.l2(pred)\n",
        "    return pred\n",
        "\n",
        "model = NeuralNet(in_features,hidden_size,out_features)\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=l_rt)\n",
        "\n",
        "for e in range(num_epochs):\n",
        "  for smp,lbl in trn_loader:\n",
        "    smp = smp.reshape(-1,28*28)\n",
        "    pred = model(smp)\n",
        "    l = loss(pred,lbl)\n",
        "    optimizer.zero_grad()\n",
        "    l.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "with torch.no_grad():\n",
        "  n_cor = 0\n",
        "  n_sap = 0\n",
        "  for img,lbl in tst_loader:\n",
        "    img = img.reshape(-1,28*28)\n",
        "    pred = model(img)\n",
        "    _,pred = torch.max(pred.data,1)\n",
        "    n_sap += lbl.shape[0]\n",
        "    n_cor += (pred==lbl).sum().item()\n",
        "  acc = 100.0*n_cor/n_sap\n",
        "  print(acc)"
      ],
      "metadata": {
        "id": "GgLsLA8V25UD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e43a516-c379-4ce4-c23a-a61249b54f20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 16.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 488kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.74MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 404: Not Found\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.24MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
            "95.26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Bad CNN only for learning how to setup a CNN\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Hyper-parameters\n",
        "num_epochs = 1\n",
        "batch_size = 200\n",
        "learning_rate = 0.001\n",
        "\n",
        "# dataset has PILImage images of range [0, 1].\n",
        "# We transform them to Tensors of normalized range [-1, 1]\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size,shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size,shuffle=False)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "class MyCnn(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MyCnn,self).__init__()\n",
        "    self.cl1 = nn.Conv2d(3,8,5)\n",
        "    self.pl1 = nn.MaxPool2d(4,4)\n",
        "    self.cl2 = nn.Conv2d(8,16,5)\n",
        "    self.pl2 = nn.MaxPool2d(2,2)\n",
        "    self.act = nn.ReLU()\n",
        "    self.act2 = nn.Tanh()\n",
        "    self.fc1 = nn.Linear(16,100)\n",
        "    self.fc2 = nn.Linear(100,70)\n",
        "    self.fc3 = nn.Linear(70,10)\n",
        "\n",
        "  def forward(self,x):\n",
        "    pred = self.cl1(x)\n",
        "    pred = self.act2(pred)\n",
        "    pred = self.pl1(pred)\n",
        "    pred = self.cl2(pred)\n",
        "    pred = self.act2(pred)\n",
        "    pred = self.pl2(pred)\n",
        "    pred = pred.reshape(pred.shape[0],-1)\n",
        "    #print(pred.shape)\n",
        "    pred = self.fc1(pred)\n",
        "    pred = self.act(pred)\n",
        "    pred = self.fc2(pred)\n",
        "    pred = self.act(pred)\n",
        "    pred = self.fc3(pred)\n",
        "    return pred #no softmax because CrossEntropyLoss() automatically does it\n",
        "\n",
        "model = MyCnn()\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
        "\n",
        "for e in range(num_epochs):\n",
        "  for i,(smp,lbl) in enumerate(train_loader):\n",
        "    pred = model(smp)\n",
        "    l = loss(pred,lbl)\n",
        "    optimizer.zero_grad()\n",
        "    l.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"PIter = {i}, Loss = {l}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "  n_correct = 0\n",
        "  n_samples = 0\n",
        "  for images, labels in test_loader:\n",
        "      outputs = model(images)\n",
        "      _, predicted = torch.max(outputs, 1)\n",
        "      n_samples += labels.size(0)\n",
        "      n_correct += (predicted == labels).sum().item()\n",
        "\n",
        "  acc = 100.0 * n_correct / n_samples\n",
        "  print(f'Accuracy of the network: {acc} %')"
      ],
      "metadata": {
        "id": "th14LsuW0212",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7102292-a86e-4d48-c82c-87cf86cebfea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "PIter = 0, Loss = 2.305555820465088\n",
            "PIter = 1, Loss = 2.3069732189178467\n",
            "PIter = 2, Loss = 2.3078315258026123\n",
            "PIter = 3, Loss = 2.306854009628296\n",
            "PIter = 4, Loss = 2.3016655445098877\n",
            "PIter = 5, Loss = 2.299232006072998\n",
            "PIter = 6, Loss = 2.301478385925293\n",
            "PIter = 7, Loss = 2.2942872047424316\n",
            "PIter = 8, Loss = 2.2893900871276855\n",
            "PIter = 9, Loss = 2.2868826389312744\n",
            "PIter = 10, Loss = 2.290618896484375\n",
            "PIter = 11, Loss = 2.2757112979888916\n",
            "PIter = 12, Loss = 2.278735876083374\n",
            "PIter = 13, Loss = 2.2676897048950195\n",
            "PIter = 14, Loss = 2.2665085792541504\n",
            "PIter = 15, Loss = 2.2573466300964355\n",
            "PIter = 16, Loss = 2.2531111240386963\n",
            "PIter = 17, Loss = 2.258697271347046\n",
            "PIter = 18, Loss = 2.2471930980682373\n",
            "PIter = 19, Loss = 2.2610340118408203\n",
            "PIter = 20, Loss = 2.245774269104004\n",
            "PIter = 21, Loss = 2.2468371391296387\n",
            "PIter = 22, Loss = 2.2179503440856934\n",
            "PIter = 23, Loss = 2.2092299461364746\n",
            "PIter = 24, Loss = 2.2194063663482666\n",
            "PIter = 25, Loss = 2.206639289855957\n",
            "PIter = 26, Loss = 2.1847524642944336\n",
            "PIter = 27, Loss = 2.2076234817504883\n",
            "PIter = 28, Loss = 2.159512758255005\n",
            "PIter = 29, Loss = 2.206275701522827\n",
            "PIter = 30, Loss = 2.1599280834198\n",
            "PIter = 31, Loss = 2.1535887718200684\n",
            "PIter = 32, Loss = 2.1526944637298584\n",
            "PIter = 33, Loss = 2.1216139793395996\n",
            "PIter = 34, Loss = 2.160907506942749\n",
            "PIter = 35, Loss = 2.1535537242889404\n",
            "PIter = 36, Loss = 2.130378007888794\n",
            "PIter = 37, Loss = 2.0738654136657715\n",
            "PIter = 38, Loss = 2.1126108169555664\n",
            "PIter = 39, Loss = 2.0961437225341797\n",
            "PIter = 40, Loss = 2.132274627685547\n",
            "PIter = 41, Loss = 2.1922898292541504\n",
            "PIter = 42, Loss = 2.1003170013427734\n",
            "PIter = 43, Loss = 2.050854206085205\n",
            "PIter = 44, Loss = 2.0393917560577393\n",
            "PIter = 45, Loss = 2.0867691040039062\n",
            "PIter = 46, Loss = 2.0626718997955322\n",
            "PIter = 47, Loss = 2.074516534805298\n",
            "PIter = 48, Loss = 2.0463457107543945\n",
            "PIter = 49, Loss = 2.0153920650482178\n",
            "PIter = 50, Loss = 2.0250842571258545\n",
            "PIter = 51, Loss = 1.975559949874878\n",
            "PIter = 52, Loss = 2.006263256072998\n",
            "PIter = 53, Loss = 2.053511381149292\n",
            "PIter = 54, Loss = 1.99424147605896\n",
            "PIter = 55, Loss = 1.976474642753601\n",
            "PIter = 56, Loss = 2.033564805984497\n",
            "PIter = 57, Loss = 1.9531735181808472\n",
            "PIter = 58, Loss = 1.956326961517334\n",
            "PIter = 59, Loss = 2.0017001628875732\n",
            "PIter = 60, Loss = 2.0004780292510986\n",
            "PIter = 61, Loss = 1.9930336475372314\n",
            "PIter = 62, Loss = 2.0038001537323\n",
            "PIter = 63, Loss = 1.9010215997695923\n",
            "PIter = 64, Loss = 1.9669222831726074\n",
            "PIter = 65, Loss = 1.9352009296417236\n",
            "PIter = 66, Loss = 1.971005916595459\n",
            "PIter = 67, Loss = 1.878888726234436\n",
            "PIter = 68, Loss = 1.8377177715301514\n",
            "PIter = 69, Loss = 1.864457607269287\n",
            "PIter = 70, Loss = 1.8871488571166992\n",
            "PIter = 71, Loss = 1.8949507474899292\n",
            "PIter = 72, Loss = 1.8909105062484741\n",
            "PIter = 73, Loss = 1.9706406593322754\n",
            "PIter = 74, Loss = 1.9176826477050781\n",
            "PIter = 75, Loss = 1.8745524883270264\n",
            "PIter = 76, Loss = 1.8696954250335693\n",
            "PIter = 77, Loss = 1.824328899383545\n",
            "PIter = 78, Loss = 1.810544490814209\n",
            "PIter = 79, Loss = 1.8228962421417236\n",
            "PIter = 80, Loss = 1.9338042736053467\n",
            "PIter = 81, Loss = 1.8458049297332764\n",
            "PIter = 82, Loss = 1.8256139755249023\n",
            "PIter = 83, Loss = 1.7883954048156738\n",
            "PIter = 84, Loss = 1.7824444770812988\n",
            "PIter = 85, Loss = 1.9063935279846191\n",
            "PIter = 86, Loss = 1.8707270622253418\n",
            "PIter = 87, Loss = 1.8217713832855225\n",
            "PIter = 88, Loss = 1.830164909362793\n",
            "PIter = 89, Loss = 1.8772189617156982\n",
            "PIter = 90, Loss = 1.816971778869629\n",
            "PIter = 91, Loss = 1.8787508010864258\n",
            "PIter = 92, Loss = 1.8233673572540283\n",
            "PIter = 93, Loss = 1.8328286409378052\n",
            "PIter = 94, Loss = 1.8593233823776245\n",
            "PIter = 95, Loss = 1.8293994665145874\n",
            "PIter = 96, Loss = 1.858710765838623\n",
            "PIter = 97, Loss = 1.8001774549484253\n",
            "PIter = 98, Loss = 1.9214786291122437\n",
            "PIter = 99, Loss = 1.7589569091796875\n",
            "PIter = 100, Loss = 1.7383935451507568\n",
            "PIter = 101, Loss = 1.8197920322418213\n",
            "PIter = 102, Loss = 1.7856923341751099\n",
            "PIter = 103, Loss = 1.7936040163040161\n",
            "PIter = 104, Loss = 1.7476240396499634\n",
            "PIter = 105, Loss = 1.8082810640335083\n",
            "PIter = 106, Loss = 1.7949680089950562\n",
            "PIter = 107, Loss = 1.749136209487915\n",
            "PIter = 108, Loss = 1.7308520078659058\n",
            "PIter = 109, Loss = 1.6944717168807983\n",
            "PIter = 110, Loss = 1.801969289779663\n",
            "PIter = 111, Loss = 1.721498727798462\n",
            "PIter = 112, Loss = 1.7193371057510376\n",
            "PIter = 113, Loss = 1.7922881841659546\n",
            "PIter = 114, Loss = 1.7507740259170532\n",
            "PIter = 115, Loss = 1.743628978729248\n",
            "PIter = 116, Loss = 1.7464488744735718\n",
            "PIter = 117, Loss = 1.8077784776687622\n",
            "PIter = 118, Loss = 1.960823655128479\n",
            "PIter = 119, Loss = 1.6964454650878906\n",
            "PIter = 120, Loss = 1.7087472677230835\n",
            "PIter = 121, Loss = 1.7211664915084839\n",
            "PIter = 122, Loss = 1.6875147819519043\n",
            "PIter = 123, Loss = 1.750839114189148\n",
            "PIter = 124, Loss = 1.7160766124725342\n",
            "PIter = 125, Loss = 1.7442442178726196\n",
            "PIter = 126, Loss = 1.6519906520843506\n",
            "PIter = 127, Loss = 1.7382867336273193\n",
            "PIter = 128, Loss = 1.7904689311981201\n",
            "PIter = 129, Loss = 1.6853293180465698\n",
            "PIter = 130, Loss = 1.8115057945251465\n",
            "PIter = 131, Loss = 1.7398594617843628\n",
            "PIter = 132, Loss = 1.7184299230575562\n",
            "PIter = 133, Loss = 1.6128944158554077\n",
            "PIter = 134, Loss = 1.702675223350525\n",
            "PIter = 135, Loss = 1.6275204420089722\n",
            "PIter = 136, Loss = 1.7012118101119995\n",
            "PIter = 137, Loss = 1.7951442003250122\n",
            "PIter = 138, Loss = 1.6588057279586792\n",
            "PIter = 139, Loss = 1.7380450963974\n",
            "PIter = 140, Loss = 1.7511235475540161\n",
            "PIter = 141, Loss = 1.5938109159469604\n",
            "PIter = 142, Loss = 1.7262272834777832\n",
            "PIter = 143, Loss = 1.6637535095214844\n",
            "PIter = 144, Loss = 1.6651489734649658\n",
            "PIter = 145, Loss = 1.6266523599624634\n",
            "PIter = 146, Loss = 1.7918715476989746\n",
            "PIter = 147, Loss = 1.749631404876709\n",
            "PIter = 148, Loss = 1.6688930988311768\n",
            "PIter = 149, Loss = 1.8441888093948364\n",
            "PIter = 150, Loss = 1.6876033544540405\n",
            "PIter = 151, Loss = 1.7073243856430054\n",
            "PIter = 152, Loss = 1.7331470251083374\n",
            "PIter = 153, Loss = 1.7277759313583374\n",
            "PIter = 154, Loss = 1.70845627784729\n",
            "PIter = 155, Loss = 1.6494991779327393\n",
            "PIter = 156, Loss = 1.7140921354293823\n",
            "PIter = 157, Loss = 1.6573805809020996\n",
            "PIter = 158, Loss = 1.643775224685669\n",
            "PIter = 159, Loss = 1.5911164283752441\n",
            "PIter = 160, Loss = 1.674749732017517\n",
            "PIter = 161, Loss = 1.6624114513397217\n",
            "PIter = 162, Loss = 1.667771816253662\n",
            "PIter = 163, Loss = 1.7508741617202759\n",
            "PIter = 164, Loss = 1.7088977098464966\n",
            "PIter = 165, Loss = 1.740938425064087\n",
            "PIter = 166, Loss = 1.7123123407363892\n",
            "PIter = 167, Loss = 1.5609869956970215\n",
            "PIter = 168, Loss = 1.6253933906555176\n",
            "PIter = 169, Loss = 1.7171894311904907\n",
            "PIter = 170, Loss = 1.6860051155090332\n",
            "PIter = 171, Loss = 1.7007216215133667\n",
            "PIter = 172, Loss = 1.6712406873703003\n",
            "PIter = 173, Loss = 1.6574095487594604\n",
            "PIter = 174, Loss = 1.6613898277282715\n",
            "PIter = 175, Loss = 1.7475835084915161\n",
            "PIter = 176, Loss = 1.6609268188476562\n",
            "PIter = 177, Loss = 1.7446277141571045\n",
            "PIter = 178, Loss = 1.725675106048584\n",
            "PIter = 179, Loss = 1.6190729141235352\n",
            "PIter = 180, Loss = 1.6463688611984253\n",
            "PIter = 181, Loss = 1.5863436460494995\n",
            "PIter = 182, Loss = 1.5381029844284058\n",
            "PIter = 183, Loss = 1.711815357208252\n",
            "PIter = 184, Loss = 1.7048540115356445\n",
            "PIter = 185, Loss = 1.681763768196106\n",
            "PIter = 186, Loss = 1.6010404825210571\n",
            "PIter = 187, Loss = 1.6176797151565552\n",
            "PIter = 188, Loss = 1.6930184364318848\n",
            "PIter = 189, Loss = 1.654903531074524\n",
            "PIter = 190, Loss = 1.7393608093261719\n",
            "PIter = 191, Loss = 1.696081280708313\n",
            "PIter = 192, Loss = 1.7113158702850342\n",
            "PIter = 193, Loss = 1.7696248292922974\n",
            "PIter = 194, Loss = 1.6359938383102417\n",
            "PIter = 195, Loss = 1.5865615606307983\n",
            "PIter = 196, Loss = 1.6337480545043945\n",
            "PIter = 197, Loss = 1.684654951095581\n",
            "PIter = 198, Loss = 1.6740124225616455\n",
            "PIter = 199, Loss = 1.583755373954773\n",
            "PIter = 200, Loss = 1.7361626625061035\n",
            "PIter = 201, Loss = 1.5867615938186646\n",
            "PIter = 202, Loss = 1.6974481344223022\n",
            "PIter = 203, Loss = 1.7408767938613892\n",
            "PIter = 204, Loss = 1.5878247022628784\n",
            "PIter = 205, Loss = 1.6593239307403564\n",
            "PIter = 206, Loss = 1.516321063041687\n",
            "PIter = 207, Loss = 1.717259407043457\n",
            "PIter = 208, Loss = 1.6991468667984009\n",
            "PIter = 209, Loss = 1.5913054943084717\n",
            "PIter = 210, Loss = 1.5897302627563477\n",
            "PIter = 211, Loss = 1.6171636581420898\n",
            "PIter = 212, Loss = 1.6401591300964355\n",
            "PIter = 213, Loss = 1.595002293586731\n",
            "PIter = 214, Loss = 1.688185691833496\n",
            "PIter = 215, Loss = 1.5341168642044067\n",
            "PIter = 216, Loss = 1.5804136991500854\n",
            "PIter = 217, Loss = 1.6870497465133667\n",
            "PIter = 218, Loss = 1.5119305849075317\n",
            "PIter = 219, Loss = 1.7173043489456177\n",
            "PIter = 220, Loss = 1.7218917608261108\n",
            "PIter = 221, Loss = 1.6200860738754272\n",
            "PIter = 222, Loss = 1.659589409828186\n",
            "PIter = 223, Loss = 1.5765057802200317\n",
            "PIter = 224, Loss = 1.6363307237625122\n",
            "PIter = 225, Loss = 1.5833210945129395\n",
            "PIter = 226, Loss = 1.7007697820663452\n",
            "PIter = 227, Loss = 1.5802197456359863\n",
            "PIter = 228, Loss = 1.6502643823623657\n",
            "PIter = 229, Loss = 1.6267752647399902\n",
            "PIter = 230, Loss = 1.5672074556350708\n",
            "PIter = 231, Loss = 1.5976958274841309\n",
            "PIter = 232, Loss = 1.7012090682983398\n",
            "PIter = 233, Loss = 1.6288647651672363\n",
            "PIter = 234, Loss = 1.5929609537124634\n",
            "PIter = 235, Loss = 1.6038099527359009\n",
            "PIter = 236, Loss = 1.583482027053833\n",
            "PIter = 237, Loss = 1.6004366874694824\n",
            "PIter = 238, Loss = 1.6735554933547974\n",
            "PIter = 239, Loss = 1.6476449966430664\n",
            "PIter = 240, Loss = 1.5486074686050415\n",
            "PIter = 241, Loss = 1.510472297668457\n",
            "PIter = 242, Loss = 1.630847454071045\n",
            "PIter = 243, Loss = 1.6088306903839111\n",
            "PIter = 244, Loss = 1.5682040452957153\n",
            "PIter = 245, Loss = 1.593511700630188\n",
            "PIter = 246, Loss = 1.6367461681365967\n",
            "PIter = 247, Loss = 1.695329189300537\n",
            "PIter = 248, Loss = 1.6117379665374756\n",
            "PIter = 249, Loss = 1.7699295282363892\n",
            "Accuracy of the network: 41.61 %\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}